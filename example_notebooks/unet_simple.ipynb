{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4zCPfG4HmmP"
   },
   "source": [
    "# `Segnet`: Medical image segmentation\n",
    "\n",
    "`segnet` is a package that makes it very easy to train, evaluate and analyse image segmentation tasks. By using a high-level API, `segnet` can be used to create all kinds of architectures and designs, all custom-built for the end user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTSlOFpGIiC1"
   },
   "source": [
    "## _Case study:_ The ISBI 2012 image segmentation challenge.\n",
    "\n",
    "[The ISBI 2012 image segmentation challenge](http://brainiac2.mit.edu/isbi_challenge/home) was launched to motivate people to create machine learning algorithms that could succesfully segment histological images.\n",
    "\n",
    "In this challenge, a full stack of electronic microscopy (EM) slices were released to train machine learning algorithms for the purpose of automatic segmentation of neural structures.\n",
    "\n",
    "In this notebook we will use this dataset to train a [U-Net](https://arxiv.org/abs/1505.04597), a fully convolutional neural network designed specifically for image segmentation tasks. We will create a full pipeline to demonstrate how to process the information first and then use `segnet` to create the architecture for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnoosYHQJnGz"
   },
   "source": [
    "## 1. Installation and imports\n",
    "\n",
    "First, we upgrade `pip`, install the latest version of `segnet`, and update `tensorflow`.\n",
    "\n",
    "I use `pip`'s quiet mode to avoid having a verbose output with the `-q` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijfqutNNKFAU"
   },
   "source": [
    "We now import the basic tools that we will need to process everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "colab_type": "code",
    "id": "3_d9k48BDreI",
    "outputId": "eb6476ab-5a07-4901-9b37-80c213c921e4"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "from segnet.models import Unet\n",
    "from segnet import utils as sut\n",
    "from segnet.metrics import metrics as mts\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhRvAZWCKMrG"
   },
   "source": [
    "## 2. Extract data and process it.\n",
    "\n",
    "We now proceed to extract the data from the official site, in this case we have already done this and saved the dataset in a Google Drive account. To actually download this one must create an account in the [home page](http://brainiac2.mit.edu/isbi_challenge/home) of the challenge.\n",
    "\n",
    "The dataset consists of 30 images and its segmentation masks, as defined by the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGnEYyTyPwlQ"
   },
   "source": [
    "### Training parameters\n",
    "In this case we define only 30 training epochs and a batch size of 8, this is after all a small dataset with only 30 training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2qGJoI_DbzM"
   },
   "outputs": [],
   "source": [
    "# Define some hyper-paramaters\n",
    "epochs = 30\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDYKdm5iKw0E"
   },
   "source": [
    "`extract_data` is a utility from `segnet` that makes it extremely easy to turn entire directory structures into `numpy` arrays. This function can handle almot every possible format and codification by using `scikit-image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHFpBw3YDbzU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30, 512, 1, 512)\n",
      "(1, 30, 512, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "# Extract images from directory\n",
    "X, y = sut.extract_data(\n",
    "    \"dataset/train_images/train-volume.tif\",\n",
    "    \"dataset/train_images/train-labels.tif\"\n",
    ")\n",
    "# Let's quickly look at the result\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icrWESBxMA7P"
   },
   "source": [
    "It seems like the encoding format for the images has created some singleton dimensions, but the information for the images is correct, we do have 30 images, each with a dimension of 512 by 512, in grayscale.\n",
    "\n",
    "Let us remove these redundant dimensions before going any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVGaFYjdKtUv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 512, 512)\n",
      "(30, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "X = np.squeeze(X)\n",
    "y = np.squeeze(y)\n",
    "# Much better!\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPszcuqRM7BC"
   },
   "source": [
    "## 3. Crop images\n",
    "\n",
    "In reality, we need smaller images for a neural network because of memory and hardware constraints. 512 by 512 images are too big, but using `segnet` we can `split_images` into smaller, randomly cropped images of the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3vNR7c4Dbzb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 128, 128, 1)\n",
      "(120, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "x_patches, y_patches = sut.split_images(\n",
    "    X, y, size=(128, 128), num_part=4\n",
    ")\n",
    "print(x_patches.shape)\n",
    "print(y_patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAkCZwxTNlgl"
   },
   "source": [
    "We now have 120, 128 by 128, grayscale images from the original 30. This is helpful, because we have quadruplicated our dataset. This will help prevent overfitting and have a better generalization.\n",
    "\n",
    "In this case there is a new singleton dimension, but this one we'll keep because it tells us about the **color channels** of the images, i.e. 1 means that the images are _actually_ grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSJ-oKKiOwGa"
   },
   "source": [
    "## 4. Train and validation sets\n",
    "\n",
    "Now it is time to split the dataset into training and validation sets, using `scikit-learn` we can do this in one line.\n",
    "\n",
    "We specify that we want 70% of the full dataset, i.e. 84 images, for the **training** set, and the other 36 for the **validation** set. We also specify a _random seed_ for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uh3e6z0hiyPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 128, 128, 1) (84, 128, 128, 1)\n",
      "(36, 128, 128, 1) (36, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(x_patches, y_patches,\n",
    "                                                 test_size=0.3,\n",
    "                                                 random_state=298)\n",
    "# Check the number of images and their segmentation maps for each set\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnZJXaILP_DG"
   },
   "source": [
    "## 5. Data augmentation\n",
    "\n",
    "Even though we created more images by cropping theme, this is not enough. To further create more images that can be useful for training purposes we need to _augment_ the data we have by using some elementary image transformation, such as rotation, translations, shifts and so on.\n",
    "\n",
    "This does not only expand our dataset, but will make the architecture robust to this transformations.\n",
    "\n",
    "Keep in mind we will only transform the **training** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zt9UxMp9Dbzk"
   },
   "outputs": [],
   "source": [
    "transformations = dict(\n",
    "    # randomly rotate images in the range (deg 0 to 180)\n",
    "    rotation_range=25.0,\n",
    "    # randomly shift images horizontally\n",
    "    width_shift_range=0.05,\n",
    "    # randomly shift images vertically\n",
    "    height_shift_range=0.05,\n",
    "    # set range for random shear\n",
    "    shear_range=50,\n",
    "    # set range for random zoom\n",
    "    zoom_range=0.2,\n",
    "    fill_mode=\"constant\",\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKibF_3fQiPD"
   },
   "source": [
    "Now we will use `segnet` to create training _generator_ based off of these transformations. The utility `image_mask_augmentation` makes it very easy to achieve this task.\n",
    "\n",
    "These transformations are performed _on the fly_, i.e. while the training loop is performed the images are transformed randomly with the specified transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_eNdKZfDbzu"
   },
   "outputs": [],
   "source": [
    "# Transformed traning set\n",
    "training = sut.image_mask_augmentation(X_train, y_train,\n",
    "                                       batch_size=batch_size,   \n",
    "                                       transformations=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7HdR1RiRccR"
   },
   "source": [
    "## 6. Architecture setup\n",
    "\n",
    "We'll now define some useful callback provided by the `keras` API, namely a model checkpoint to always keep a copy of the best trained model, as well as a learning rate reducer that will help the architecture converge faster when reaching a plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FVgiFTADbzz"
   },
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "save_best_model = K.callbacks.ModelCheckpoint(\"unet_simple_new.h5\",\n",
    "                                 monitor=\"val_jaccard_index\",\n",
    "                                 mode=\"max\",\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True,\n",
    "                                )\n",
    "csv_logger = K.callbacks.CSVLogger(\"unet_simple_new.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECZyrcdrR2mW"
   },
   "source": [
    "## 7. Architecture specification\n",
    "\n",
    "The U-Net is a very specific architecture, and `segnet` has an implementation of it. It follows the original paper closely, and this implementation provides a _simple_ variant and a _custom_ variant. We will be using the _simple_ variant here, i.e. the one defined in the original paper.\n",
    "\n",
    "For metrics we chose the [jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) which is a very standard metric for segmentation tasks.\n",
    "\n",
    "For the loss function, we will employ binary crossentropy, as we only have two possible classes here, either a border or not whitin the segmentation masks. \n",
    "\n",
    "For the optimizer we chose the robust Stochastic Gradient Descent with classical momentum and a large learning rate to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0xURaznDbz2"
   },
   "outputs": [],
   "source": [
    "# U-net\n",
    "model_unet = Unet(input_size=(128, 128, 1), variant=\"simple\").collect()\n",
    "# This is now a `Keras model`\n",
    "model_unet.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=K.optimizers.Nadam(),\n",
    "    metrics=[mts.jaccard_index, mts.o_rate, mts.u_rate, mts.err_rate],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPO_za3kDbz6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"unet_simple\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 1024)   4719616     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 1024)   9438208     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 16, 16, 1024) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 512)  2097664     up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 1024) 0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 512)  4719104     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 512)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  524544      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 512)  0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  131200      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 256)  0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 128 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 64) 32832       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 128 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 128, 128, 2)  1154        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 128, 128, 1)  3           conv2d_22[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,031,685\n",
      "Trainable params: 31,031,685\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kUC8pxpS65m"
   },
   "source": [
    "This is a _big_ network, 31+ million parameters to train, but worry not, because although we might be prone to some overfitting, the generalization of the network will be quite fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K44gkq4aTRL_"
   },
   "source": [
    "## 7. Training\n",
    "\n",
    "This is the big moment, let us now train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7domTjH8Dbz_"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NumpyArrayIterator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5e79fc1fb6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/segnet/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/segnet/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       shuffle=shuffle)\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/segnet/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mconvert_to_generator_like\u001b[0;34m(data, batch_size, steps_per_epoch, epochs, shuffle)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m   \u001b[0;31m# Create generator from NumPy or EagerTensor Input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m   \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     raise ValueError(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NumpyArrayIterator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "history = model_unet.fit_generator(\n",
    "    # Extract the tuples from the generator\n",
    "    ((i) for i in training),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=20,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    "    callbacks=[save_best_model, csv_logger],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "55hqTwzkj0CN",
    "outputId": "9cfdb0b3-bdee-46f2-bb74-c967430dfb52"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(history.history[\"jaccard_index\"])\n",
    "plt.plot(history.history[\"val_jaccard_index\"])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"Jaccard index\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "jLO9_8t2j47F",
    "outputId": "1ac2fd03-ad0e-4415-ac0c-40cfe12f3cf8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArZ0CTEakcZg"
   },
   "outputs": [],
   "source": [
    "X_test = sut.extract_data(\"dataset/test_images/test-volume.tif\")\n",
    "X_test = np.squeeze(X_test)\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_patches = sut.split_images( X_test, size=(128, 128), num_part=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tK66PmLWkxLu"
   },
   "outputs": [],
   "source": [
    "model_unet.load_weights(\"unet_simple_new.h5\")\n",
    "results = model_unet.predict(x_test_patches, batch_size=batch_size)\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "thresh = threshold_otsu(X_test[1, ...])\n",
    "binary = X_test[1, ...] > thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3w6nBNCNlJRz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.imshow(binary[..., 0], cmap=\"gray\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(X_test[1, :, :, 0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unet_simple.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
