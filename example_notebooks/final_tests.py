# -*- coding: utf-8 -*-
"""Final_tests.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tm1i32ADm2IDNZqFduqOhveSY93MZjKJ
"""

!ls drive

!apt install jq
!pip install git+https://github.com/gmagannaDevelop/segnet.git@mru_tests

import os

###############################################################
import tensorflow as tf

import segnet.metrics as mts
 
from segnet.models import unet
from segnet.models import multiresunet as mru
from segnet.models import multiresunet2 as mru2
from segnet.models import multiresunet3 as mru3
from segnet.utils.Segmed import Segmed

### Data-related
from google.colab import drive, files
drive.mount('/content/drive/')
###############################################################
### Not an import, but mandatory to be defined here :
root_dir = "drive/My Drive/Gus_Servicio_Profesional"
log_dir  = os.path.join(root_dir, "Logs")

dataset_paths = { 
  "isbi":          "drive/My Drive/Gus_Servicio_Profesional/Colab_data/ISBI_neural/structured", 
  "colonoscopy":   "drive/My Drive/Gus_Servicio_Profesional/Colab_data/colonoscopy",   # Full original
  "dermoscopy80":  "drive/My Drive/Gus_Servicio_Profesional/Colab_data/dermoscopy80",  # reduced to 80 images
  "dermoscopy150": "drive/My Drive/Gus_Servicio_Profesional/Colab_data/dermoscopy150", # reduced to 150, 
  "chinese1":      "drive/My Drive/Gus_Servicio_Profesional/Colab_data/Dataset 2"      # Chinese dataset
}

optimizers = {
    "chinese": tf.keras.optimizers.SGD(learning_rate=0.06, momentum=0.2, nesterov=False),
    "Original Adam": tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, epsilon=10e-8)
}

my_compiling_kw = {
    'optimizer': optimizers["Original Adam"],
    'loss': 'binary_crossentropy',
    'metrics': [
      mts.jaccard_index, mts.dice_coef,
      mts.O_Rate, mts.U_Rate, mts.Err_rate
    ]
}

dataset = dataset_paths["dermoscopy80"]

len(os.listdir(os.path.join(dataset, "msks/masks")))

my_hyper_params = {
    'batch_size': 20,
    'epochs': 15,
    'steps_per_epoch': 4
}

architectures = {
  "Unet": unet(),
  "MultiResUNet Edwin": mru.MultiResUnet(),
  "MultiResUNet Gustavo": mru2.MultiResUNet(),
  "MultiResUNet Original": mru3.MultiResUnet()
}

models = {
    key: Segmed(
        model = architectures[key],
        name = key,
        base_dir = log_dir,
        data_path = dataset_paths["dermoscopy80"],
        author = "Gustavo Maga√±a"
    )
    for key in architectures.keys()
}

models

for model in models.values():
  model.comment(" Retrying using GPU accelerator, few epochs and the smallest dataset that I have. ")

for model in models.values():
  model.train(
    compiling_kw = my_compiling_kw,
    hyper_params = my_hyper_params
  )

